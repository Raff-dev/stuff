{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"~/projects/personal/inferred\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    f\"{repo_path}/backend\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(disallowed_special=())\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\") \n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what are the endpoints available?\"\n",
    "# result = qa(question)\n",
    "# result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Project Inferred: A Masterpiece for Masters \\n\\nHello there, fellow tech aficionado!\\n\\nWelcome to the GitHub repository of *Project Inferred* — the pièce de résistance of my Master\\'s degree thesis. This project is a perfect blend of technology, humor, and more importantly, top-notch coding practices, all served on a platter of Django and Python. So, grab a cup of your favorite beverage, and let\\'s dive right in!\\n\\n## Introduction\\n\\nProject Inferred is a dashboard for assessing the adequacy of simulation models in digital twin applications. It\\'s like the Sherlock Holmes of the digital universe, always on the lookout for discrepancies in digital twins. If you are wondering, \"Digital what...?\", let me explain. Digital twins are virtual replicas of physical entities that data scientists and IT pros can use to run simulations before actual devices are built and deployed. And our project is kind of a detective that ensures these twins are not going rogue. Cool, right?\\n\\n## Tech Stack \\n\\nThis project is written in Python and uses Django for the web framework. We use Django\\'s models for databases and Channels for handling WebSockets. Redis is used as the message broker, and ASGI is used for handling asynchronous requests. Lastly, the dash of humor you see here is purely Pythonic!\\n\\n## Getting Started \\n\\nNow that you\\'re on board with what we\\'re doing, let\\'s get down to brass tacks. Here are the steps to set up and run this project:\\n\\n**Step 1:** Clone the repository\\n\\n    git clone https://github.com/yourusername/project-inferred.git\\n\\n**Step 2:** Create a virtual environment and activate it\\n\\n    python -m venv env\\n    source env/bin/activate  # On Windows, use `.\\\\env\\\\Scripts\\\\activate`\\n\\n**Step 3:** Install the required packages\\n\\n    pip install -r requirements.txt\\n\\n**Step 4:** Set up the database\\n\\n    python manage.py makemigrations\\n    python manage.py migrate \\n\\n**Step 5:** Run the server\\n\\n    python manage.py runserver\\n\\nVoila! You\\'re all set to explore Project Inferred.\\n\\n## Conclusion\\n\\nThis project is not just a requirement for my Master\\'s degree but also a testament to my love for coding and problem-solving. I\\'ve put my heart, soul, and a considerable amount of coffee into this project. I hope you\\'ll appreciate the work and find it useful for your purposes.\\n\\nIf you have any queries, suggestions, or if you just want to share your favorite Python joke, feel free to reach out.\\n\\nHappy Exploring!\\n\\nP.S. - Remember, \"Talk is cheap. Show me the code.\" - Linus Torvalds\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Write a fancy, funny, but not cringe readme for this project. explain steps to setup and run the project. explain that it is a project for my masters degree thesis. explain the stack. topic of the thesis is 'Dashboard for Assessing the Adequacy of Simulation Models in Digital Twin Applications'\"\n",
    "result = qa(prompt)\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'how to run the project?',\n",
       " 'chat_history': [SystemMessage(content='', additional_kwargs={})],\n",
       " 'answer': 'The context provided does not include information on the steps to run the project.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(\"how to run the project?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ease-1no_vdUF-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
